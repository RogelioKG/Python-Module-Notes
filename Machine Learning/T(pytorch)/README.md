# pytorch

[![RogelioKG/pytorch](https://img.shields.io/badge/Sync%20with%20HackMD-grey?logo=markdown)](https://hackmd.io/@RogelioKG/pytorch)

## References
+ ğŸ”— [**Document - Pytorch**](https://pytorch.org/)

## Note

|ğŸ“— <span class="tip">TIP</span>|
|:---|
| æª¢æŸ¥ CUDA ç‰ˆæœ¬ï¼š`nvcc --version` |

|ğŸ“˜ <span class="NOTE">NOTE</span>|
|:---|
| æœ¬ç­†è¨˜ç›®å‰æš«æ™‚ä»¥ LLM è‡ªå‹•ç”¢å‡ºï¼Œæœªç¶“éæ¶ˆåŒ–æ•´ç† |

## 1. å®‰è£

+ ç¯„ä¾‹
  |ğŸš¨ <span class="caution">CAUTION</span>|
  |:---|
  | è«‹å®‰è£èˆ‡æœ¬æ©Ÿ CUDA ç›¸ç¬¦çš„ç‰ˆæœ¬ |
  ```toml
  # pyproject.toml
  [tool.uv.sources]
  torch = [{ index = "pytorch-cu121" }]
  torchvision = [{ index = "pytorch-cu121" }]
  torchaudio = [{ index = "pytorch-cu121" }]

  [[tool.uv.index]]
  name = "pytorch-cu121"
  url = "https://download.pytorch.org/whl/cu121"
  explicit = true
  ```
  ```ps
  uv add torch torchvision torchaudio
  ```

## 2. å¼µé‡ `Tensor`
+ èªªæ˜
  + PyTorch çš„æ ¸å¿ƒè³‡æ–™çµæ§‹æ˜¯ `Tensor`
  + å®ƒé¡ä¼¼æ–¼ NumPy çš„ `ndarray`ï¼Œä½†æœ‰å…©å€‹é—œéµå€åˆ¥
    1.  å¯ä»¥åœ¨ GPU ä¸Šé‹è¡Œä»¥åŠ é€Ÿè¨ˆç®—
    2.  å¯ä»¥å„²å­˜æ¢¯åº¦è³‡è¨Šï¼ˆç”¨æ–¼è‡ªå‹•å¾®åˆ†ï¼‰

+ ç¯„ä¾‹ 2.1 - å»ºç«‹å¼µé‡
  ```python
  import torch

  # 1. ç›´æ¥å¾æ•¸æ“šå»ºç«‹
  data = [[1, 2], [3, 4]]
  x_data = torch.tensor(data)

  # 2. éš¨æ©Ÿç”Ÿæˆèˆ‡å…¨ 0/1 å¼µé‡
  shape = (2, 3)
  rand_tensor = torch.rand(shape)  # å‡å‹»åˆ†ä½ˆ
  ones_tensor = torch.ones(shape)  # å…¨ 1
  zeros_tensor = torch.zeros(shape) # å…¨ 0

  print(f"Random Tensor: \n{rand_tensor}")
  ```

+ ç¯„ä¾‹ 2.2 - å¼µé‡å±¬æ€§èˆ‡æ“ä½œ
  > äº†è§£å¼µé‡çš„å½¢ç‹€ (Shape) å’Œè£ç½® (Device) æ˜¯é™¤éŒ¯çš„é—œéµ
  ```python
  tensor = torch.rand(3, 4)

  print(f"Shape: {tensor.shape}")
  print(f"Datatype: {tensor.dtype}")
  print(f"Device: {tensor.device}") # é è¨­æ˜¯ 'cpu'

  # å¼µé‡é‹ç®— (é¡ä¼¼ NumPy)
  t1 = torch.rand(2, 2)
  t2 = torch.rand(2, 2)

  # çŸ©é™£ä¹˜æ³• (Matrix Multiplication)
  # ç­‰åŒæ–¼ t1 @ t2
  matmul_res = torch.matmul(t1, t2) 

  # å…ƒç´ å°æ‡‰ç›¸ä¹˜ (Element-wise Product)
  # ç­‰åŒæ–¼ t1 * t2
  elem_res = torch.mul(t1, t2)
  ```

+ ç¯„ä¾‹ 2.3 - GPU åŠ é€Ÿ (CUDA/MPS)
  > å°‡å¼µé‡ç§»å‹•åˆ° GPU æ˜¯åŠ é€Ÿæ·±åº¦å­¸ç¿’çš„é—œéµæ­¥é©Ÿã€‚
  ```python
  # æª¢æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„ GPU (Nvidia CUDA æˆ– Mac MPS)
  device = (
      "cuda"
      if torch.cuda.is_available()
      else "mps"
      if torch.backends.mps.is_available()
      else "cpu"
  )
  print(f"Using {device} device")

  # ç§»å‹•å¼µé‡
  tensor = tensor.to(device)
  ```

## 3. è‡ªå‹•å¾®åˆ†å¼•æ“ `autograd`

+ èªªæ˜
  + ç¥ç¶“ç¶²è·¯çš„è¨“ç·´ä¾è³´æ–¼ backpropagation
  + PyTorch çš„ `autograd` æ¨¡çµ„æœƒè‡ªå‹•è¨˜éŒ„å°å¼µé‡çš„æ‰€æœ‰æ“ä½œï¼Œä»¥ä¾¿è¨ˆç®—æ¢¯åº¦
    + è¨­å®š `requires_grad=True` ä¾†è¿½è¹¤è¨ˆç®—æ­·å²ã€‚
    + å‘¼å« `.backward()` ä¾†è‡ªå‹•è¨ˆç®—æ¢¯åº¦ã€‚
    + æ¢¯åº¦æœƒç´¯ç©åœ¨ `.grad` å±¬æ€§ä¸­ã€‚
+ ç¯„ä¾‹ 3.1
  ```python
  x = torch.ones(5)  # input
  y = torch.zeros(3)  # expected output
  w = torch.randn(5, 3, requires_grad=True) # weights (éœ€è¦æ›´æ–°)
  b = torch.randn(3, requires_grad=True)    # bias (éœ€è¦æ›´æ–°)

  # Forward pass (å‰å‘å‚³æ’­)
  z = torch.matmul(x, w) + b

  # è¨ˆç®— Loss (ä¾‹å¦‚ Binary Cross Entropy)
  loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)

  print(f"Gradient function for z: {z.grad_fn}")

  # Backward pass (åå‘å‚³æ’­)
  loss.backward()

  # æŸ¥çœ‹æ¢¯åº¦ (é€™äº›æ¢¯åº¦å°‡ç”¨æ–¼æ›´æ–° w å’Œ b)
  print(w.grad)
  print(b.grad)
  ```

## 4. ç¥ç¶“ç¶²è·¯ `Module`
+ èªªæ˜
  + `torch.nn` æä¾›äº†æ§‹å»ºç¥ç¶“ç¶²è·¯æ‰€éœ€çš„å±¤ (Layers) å’Œå®¹å™¨
  + æ‰€æœ‰çš„æ¨¡å‹éƒ½æ‡‰è©²ç¹¼æ‰¿ `nn.Module`
+ ç¯„ä¾‹
+ ç¯„ä¾‹ 4.1 - å®šç¾©ä¸€å€‹ç°¡å–®çš„æ¨¡å‹
  > æˆ‘å€‘ä¾†å»ºç«‹ä¸€å€‹æ¨™æº–çš„å¤šå±¤æ„ŸçŸ¥æ©Ÿ (MLP)
  ```python
  from torch import nn

  class NeuralNetwork(nn.Module):
      def __init__(self):
          super().__init__()
          # å®šç¾©å±¤
          self.flatten = nn.Flatten()
          self.linear_relu_stack = nn.Sequential(
              nn.Linear(28*28, 512), # è¼¸å…¥å±¤ -> éš±è—å±¤
              nn.ReLU(),             # æ¿€æ´»å‡½æ•¸
              nn.Linear(512, 512),   # éš±è—å±¤ -> éš±è—å±¤
              nn.ReLU(),
              nn.Linear(512, 10)     # éš±è—å±¤ -> è¼¸å‡ºå±¤ (10é¡)
          )

      def forward(self, x):
          # å®šç¾©æ•¸æ“šæµå‘
          x = self.flatten(x)
          logits = self.linear_relu_stack(x)
          return logits

  # å¯¦ä¾‹åŒ–æ¨¡å‹ä¸¦ç§»è‡³ GPU
  model = NeuralNetwork().to(device)
  print(model)
  ```

## 5. æ•¸æ“šè™•ç† `Dataset` & `DataLoader`
+ èªªæ˜
  + `torch.utils.data.Dataset`ï¼šå„²å­˜æ¨£æœ¬åŠå…¶å°æ‡‰æ¨™ç±¤
  + `torch.utils.data.DataLoader`ï¼šå°‡ Dataset åŒ…è£æˆä¸€å€‹å¯è¿­ä»£å°è±¡ (Iterable)ï¼Œæä¾› batching (æ‰¹æ¬¡è™•ç†)ã€shuffling (æ‰“äº‚) ç­‰åŠŸèƒ½

+ ç¯„ä¾‹ 5.1 - è‡ªå®šç¾© Dataset

  ```python
  from torch.utils.data import Dataset, DataLoader

  class CustomNumberDataset(Dataset):
      def __init__(self, length=1000):
          # åˆå§‹åŒ–æ•¸æ“š (é€™è£¡æˆ‘å€‘éš¨æ©Ÿç”Ÿæˆä¸€äº›å‡æ•¸æ“š)
          # å‡è¨­è¼¸å…¥æ˜¯ 28x28 çš„éš¨æ©Ÿåœ–åƒï¼Œæ¨™ç±¤æ˜¯ 0-9 çš„æ•´æ•¸
          self.data = torch.randn(length, 28, 28)
          self.labels = torch.randint(0, 10, (length,))

      def __len__(self):
          # å›å‚³æ•¸æ“šé›†ç¸½é•·åº¦
          return len(self.data)

      def __getitem__(self, idx):
          # æ ¹æ“šç´¢å¼•å›å‚³ä¸€ç­†æ•¸æ“š (image, label)
          return self.data[idx], self.labels[idx]

  # å»ºç«‹ Dataset
  training_data = CustomNumberDataset(length=1000)
  test_data = CustomNumberDataset(length=200)

  # å»ºç«‹ DataLoader
  batch_size = 64
  train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)
  test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)

  # æ¸¬è©¦è®€å–ä¸€å€‹ Batch
  for X, y in train_dataloader:
      print(f"Shape of X [N, C, H, W]: {X.shape}")
      print(f"Shape of y: {y.shape} {y.dtype}")
      break
  ```

## 6. è¨“ç·´å¾ªç’°

+ èªªæ˜
  + é€™æ˜¯å°‡æ‰€æœ‰ç©æœ¨çµ„åˆåœ¨ä¸€èµ·çš„æ™‚åˆ»ã€‚è¨“ç·´æ¨¡å‹åŒ…å«ä¸‰å€‹æ­¥é©Ÿï¼š
    1.  **å®šç¾©æå¤±å‡½æ•¸ (Loss Function)**: è¡¡é‡æ¨¡å‹é æ¸¬èˆ‡çœŸå¯¦å€¼çš„å·®è·ã€‚
    2.  **å®šç¾©å„ªåŒ–å™¨ (Optimizer)**: æ ¹æ“šæ¢¯åº¦æ›´æ–°æ¨¡å‹åƒæ•¸ã€‚
    3.  **è¿­ä»£è¨“ç·´**: å‰å‘å‚³æ’­ -\> è¨ˆç®— Loss -\> åå‘å‚³æ’­ -\> æ›´æ–°åƒæ•¸ã€‚

+ ç¯„ä¾‹ 6.1 - è¨­å®šè¶…åƒæ•¸èˆ‡çµ„ä»¶
  ```python
  learning_rate = 1e-3
  epochs = 5

  # æå¤±å‡½æ•¸ (åˆ†é¡å•é¡Œå¸¸ç”¨ Cross Entropy)
  loss_fn = nn.CrossEntropyLoss()

  # å„ªåŒ–å™¨ (SGD æˆ– Adam)
  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
  ```

+ ç¯„ä¾‹ 6.2 - è¨“ç·´èˆ‡æ¸¬è©¦å‡½æ•¸

  > ç‚ºäº†ä¿æŒç¨‹å¼ç¢¼æ•´æ½”ï¼Œæˆ‘å€‘å°‡è¨“ç·´å’Œæ¸¬è©¦é‚è¼¯å°è£æˆå‡½æ•¸ã€‚

  ```python
  def train_loop(dataloader, model, loss_fn, optimizer):
      size = len(dataloader.dataset)
      model.train() # è¨­å®šç‚ºè¨“ç·´æ¨¡å¼ (å•Ÿç”¨ Dropout, BatchNorm ç­‰)

      for batch, (X, y) in enumerate(dataloader):
          # 1. å°‡æ•¸æ“šç§»è‡³ GPU/MPS
          X, y = X.to(device), y.to(device)

          # 2. å‰å‘å‚³æ’­ (é æ¸¬)
          pred = model(X)
          loss = loss_fn(pred, y)

          # 3. åå‘å‚³æ’­ (Backpropagation)
          optimizer.zero_grad() # æ¸…ç©ºèˆŠçš„æ¢¯åº¦
          loss.backward()       # è¨ˆç®—æ–°æ¢¯åº¦
          optimizer.step()      # æ›´æ–°åƒæ•¸

          if batch % 5 == 0:
              loss, current = loss.item(), batch * len(X)
              print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")

  def test_loop(dataloader, model, loss_fn):
      model.eval() # è¨­å®šç‚ºè©•ä¼°æ¨¡å¼
      size = len(dataloader.dataset)
      num_batches = len(dataloader)
      test_loss, correct = 0, 0

      # è©•ä¼°æ™‚ä¸éœ€è¦è¨ˆç®—æ¢¯åº¦ï¼Œç¯€çœè¨˜æ†¶é«”
      with torch.no_grad():
          for X, y in dataloader:
              X, y = X.to(device), y.to(device)
              pred = model(X)
              test_loss += loss_fn(pred, y).item()
              correct += (pred.argmax(1) == y).type(torch.float).sum().item()

      test_loss /= num_batches
      correct /= size
      print(f"Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")
  ```

+ ç¯„ä¾‹ 6.3 - é–‹å§‹è¨“ç·´

  ```python
  print(f"Training on {device}...")
  for t in range(epochs):
      print(f"Epoch {t+1}\n-------------------------------")
      train_loop(train_dataloader, model, loss_fn, optimizer)
      test_loop(test_dataloader, model, loss_fn)
  print("Done!")
  ```


## 7. å„²å­˜ã€è¼‰å…¥æ¨¡å‹

+ èªªæ˜
  è¨“ç·´å®Œæˆå¾Œï¼Œæˆ‘å€‘é€šå¸¸æœƒå„²å­˜æ¨¡å‹çš„æ¬Šé‡ï¼ˆ`state_dict`ï¼‰ï¼Œè€Œä¸æ˜¯å„²å­˜æ•´å€‹æ¨¡å‹çµæ§‹ï¼ˆé€™æ¨£ç›¸å®¹æ€§è¼ƒå·®ï¼‰ã€‚

+ ç¯„ä¾‹ 7.1 - å„²å­˜
  ```python
  torch.save(model.state_dict(), "model_weights.pth")
  print("Saved PyTorch Model State to model_weights.pth")
  ```

+ ç¯„ä¾‹ 7.2 - è¼‰å…¥
  ```python
  # 1. å¿…é ˆå…ˆé‡æ–°å»ºç«‹æ¨¡å‹æ¶æ§‹ (å› ç‚ºæˆ‘å€‘åªå­˜äº†æ¬Šé‡)
  model2 = NeuralNetwork().to(device)

  # 2. è¼‰å…¥æ¬Šé‡
  model2.load_state_dict(torch.load("model_weights.pth", weights_only=True))

  # 3. è¨­å®šç‚ºè©•ä¼°æ¨¡å¼
  model2.eval()
  print("Model loaded successfully!")
  ```


## 8. é€²éšæŠ€å·§ã€æœ€ä½³å¯¦è¸

1. **TensorBoard è¦–è¦ºåŒ–**:
    PyTorch æ”¯æ´ TensorBoard ä¾†ç›£æ§ Loss æ›²ç·šã€‚

    ```python
    from torch.utils.tensorboard import SummaryWriter
    writer = SummaryWriter('runs/experiment_1')
    # åœ¨ train_loop ä¸­: writer.add_scalar('Loss/train', loss, epoch)
    ```

2. **é™¤éŒ¯ç¶­åº¦å•é¡Œ**:
    æ·±åº¦å­¸ç¿’æœ€å¸¸è¦‹çš„éŒ¯èª¤æ˜¯ç¶­åº¦ä¸åŒ¹é… (Shape mismatch)ã€‚

      * å–„ç”¨ `print(x.shape)`ã€‚
      * ä½¿ç”¨ `x.view()` æˆ– `x.reshape()` æ”¹è®Šå½¢ç‹€ã€‚
      * ä½¿ç”¨ `x.unsqueeze(0)` å¢åŠ  batch ç¶­åº¦ï¼ˆä¾‹å¦‚å°‡ `[3, 32, 32]` è®Šç‚º `[1, 3, 32, 32]`ï¼‰ã€‚

3. **`model.train()` vs `model.eval()`**:
    æ°¸é è¨˜å¾—åœ¨è¨“ç·´æ™‚å‘¼å« `.train()`ï¼Œåœ¨æ¸¬è©¦/æ¨è«–æ™‚å‘¼å« `.eval()`ã€‚é€™æœƒå½±éŸ¿ Dropout å’Œ Batch Normalization çš„è¡Œç‚ºã€‚

4. **ä½¿ç”¨ `torchvision.transforms`**:
    å°æ–¼å½±åƒè™•ç†ï¼Œä½¿ç”¨ Transforms é€²è¡Œæ•¸æ“šå¢å¼· (Data Augmentation) æ˜¯æ¨™æº–åšæ³•ã€‚

    ```python
    from torchvision import transforms
    transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
    ])
    ```