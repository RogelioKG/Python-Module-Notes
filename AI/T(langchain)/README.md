# langchain

[![RogelioKG/langchain](https://img.shields.io/badge/Sync%20with%20HackMD-grey?logo=markdown)](https://hackmd.io/@RogelioKG/langchain)

## References
+ ğŸ”— [**Docs - LangChain**](https://docs.langchain.com/)
+ ğŸ”— [**Docs - LangChain references**](https://reference.langchain.com/python/langchain/)
+ ğŸ”— [**GitHub - LangChain**](https://github.com/langchain-ai/langchain)
+ ğŸ”— [**GitHub - LangGraph**](https://github.com/langchain-ai/langgraph)
+ ğŸ”— [**MyApollo - LCEL ç¯‡**](https://myapollo.com.tw/blog/langchain-expression-language/)
+ ğŸ”— [**LangChain æ¡†æ¶ä»‹ç´¹ï¼šæ‰“é€  AI Agent æ™ºæ…§åŠ©ç†**](https://www.mropengate.com/2025/05/langchain-ai-agent.html)
+ ğŸ”— [**LangChain 1.0 é€Ÿé€šæŒ‡å—ï¼ˆä¸€ï¼‰â€”â€” LangChain 1.0 æ ¸å¿ƒå‡çº§**](https://zhuanlan.zhihu.com/p/1968427472388335014)
+ ğŸ”— [**LangChain 1.0 é€Ÿé€šæŒ‡å—ï¼ˆäºŒï¼‰â€”â€” LangChain 1.0 create_agent api åŸºç¡€çŸ¥è¯†**](https://zhuanlan.zhihu.com/p/1969383902624842213)
+ ğŸ”— [**LangChain 1.0 é€Ÿé€šæŒ‡å—ï¼ˆä¸‰ï¼‰â€”â€” LangChain 1.0 create_agent api é«˜é˜¶åŠŸèƒ½**](https://zhuanlan.zhihu.com/p/1970460972620679090)
+ ğŸ”— [**iT é‚¦ - ç”¨ LangGraph å¾é›¶é–‹å§‹å¯¦ç¾ Agentic AI System ç³»åˆ—**](https://ithelp.ithome.com.tw/m/users/20161074/ironman/7469)
+ ğŸ”— [**YWC ç§‘æŠ€ç­†è¨˜ - LangGraph: LangChain Agent çš„æ®ºæ‰‹é§ - Part 1**](https://ywctech.net/ml-ai/langchain-langgraph-agent-part1/)
+ ğŸ”— [**YWC ç§‘æŠ€ç­†è¨˜ - LangGraph: LangChain Agent çš„æ®ºæ‰‹é§ - Part 2**](https://ywctech.net/ml-ai/langchain-langgraph-agent-part2/)


## Runnable
+ èªªæ˜
  + åªè¦ä¸€å€‹å…ƒä»¶éµå®ˆ Runnable Protocolï¼Œå°±èƒ½è¢«è¦–ç‚º LCEL çš„ä¸€ä»½å­
  + Runnable å®¶æ—å¾ˆé…·ï¼Œå®ƒå€‘èƒ½<mark>ç”¨ `|` ä¸²æ¥æˆ pipelineï¼Œé€™å°±æ˜¯ LCEL</mark>
    ```py
    retriever | prompt | llm | parser
    ```
+ æ ¸å¿ƒæ–¹æ³•
  + invoke (æ”¯æ´å–®ä¸€è¼¸å…¥ã€å–®ä¸€è¼¸å‡ºï¼‰
  + batch (æ”¯æ´å¤šå€‹è¼¸å…¥ã€å¤šå€‹è¼¸å‡ºï¼‰
  + stream (æ”¯æ´æœ‰éƒ¨åˆ†çµæœå°±è¼¸å‡ºçš„æ¨¡å¼)
  + ainvoke (async ç‰ˆæœ¬çš„ invoke)
  + abatch (async ç‰ˆæœ¬çš„ batch)
  + astream (async ç‰ˆæœ¬çš„ stream)

## [Graph API](https://docs.langchain.com/oss/python/langgraph/graph-api)

### graph - basic info
+ ä¸‰è¦ç´ 
  + stateï¼šç‹€æ…‹
  + nodesï¼šç¯€é»
  + edgesï¼šæœ‰å‘é‚Š

### state - schema
+ èªªæ˜
  + `StateGraph` æ¡ç”¨ Builder Pattern
  + ä¸€æ­¥ä¸€æ­¥æŠŠ graph å»ºé€ å‡ºä¾†å¾Œå† compile æˆå¯¦ä¾‹
+ ç‹€æ…‹
  + é¡åˆ¥
    + `TypedDict` (ç°¡å–®)
    + `dataclass` (æ”¯æ´ default value)
    + `Pydantic BaseModel` (æ”¯æ´ verification)
  + ç¨®é¡
    + `state_schema`ï¼šå…¬æœ‰ç‹€æ…‹ (æ•´å€‹ graph ä¸­å…±äº«çš„<mark>è®Šå‹•è³‡æ–™</mark>)
    + `context_schema`ï¼šä¸Šä¸‹æ–‡ç‹€æ…‹ (æ•´å€‹ graph ä¸­å…±äº«çš„<mark>åŸºæœ¬è¨­å®š</mark>)
    + `input_schema`ï¼šè¼¸å…¥ç‹€æ…‹ (è‹¥æœªçµ¦å®šï¼Œé è¨­ç‚º `state_schema`)
    + `output_schema`ï¼šè¼¸å‡ºç‹€æ…‹ (è‹¥æœªçµ¦å®šï¼Œé è¨­ç‚º `state_schema`)
+ ç¯„ä¾‹
  |â˜¢ï¸ <span class="warning">WARNING</span> : BUG|
  |:---|
  | å·¥ä½œæµä¸ä¸€å®šå¾—æ˜¯ DAGï¼Œæ‰€ä»¥æœ‰å¯èƒ½è¨­è¨ˆå‡º cycleï¼Œæ­¤æ™‚æ¢ä»¶è‹¥æœªè¦ç¯„å¥½ï¼Œå°±æœƒé€ æˆç„¡é™è¿´åœˆ |
  | solutionï¼šåœ¨ `graph.invoke()` è£¡åŠ ä¸Š `config={"recursion_limit": n}` (æ¯å€‹ç¯€é»æœ€å¤šè™•ç† n æ¬¡ä»»å‹™)ï¼Œè¶…éå°‡å¼•ç™¼ `GraphRecursionError` |
  ```mermaid
  ---
  config:
    flowchart:
      curve: linear
  ---
  graph TD;
          __start__([<p>__start__</p>]):::first
          node_1(node_1)
          node_2(node_2)
          node_3(node_3)
          __end__([<p>__end__</p>]):::last
          __start__ --> node_1;
          node_1 --> node_2;
          node_2 --> node_3;
          node_3 --> __end__;
          classDef default fill:#f2f0ff,line-height:1.2
          classDef first fill-opacity:0
          classDef last fill:#bfb6fc
  ```
  ```py
  from typing import TypedDict

  from langgraph.graph import END, START, StateGraph
  from langgraph.runtime import Runtime


  # å…¬æœ‰ state
  class PublicState(TypedDict, total=False):
      foo: str
      user_input: str
      graph_output: str


  # ä¸Šä¸‹æ–‡ State
  class ContextState(TypedDict):
      llm_provider: str


  # è¼¸å…¥ State
  class InputState(TypedDict):
      user_input: str


  # è¼¸å‡º State
  class OutputState(TypedDict):
      graph_output: str


  # ç§æœ‰ state
  class PrivateState(TypedDict):
      bar: str


  def node_1(state: InputState) -> PublicState:
      # assert state == {"user_input": "My"}
      return {"foo": state["user_input"] + " name"}


  def node_2(state: PublicState) -> PrivateState:
      # assert state == {'foo': 'My name', 'user_input': 'My'}
      return {"bar": state["foo"] + " is"}  # type: ignore


  def node_3(state: PrivateState, runtime: Runtime[ContextState]) -> OutputState:
      # assert state == {'bar': 'My name is'}
      # assert runtime.context == {'llm_provider': 'openai'}
      return {"graph_output": state["bar"] + " Lance"}


  if __name__ == "__main__":
      # StateGraph æ¡ç”¨ Builder Pattern
      builder = StateGraph(PublicState, ContextState, input_schema=InputState, output_schema=OutputState)
      builder.add_node("node_1", node_1)
      builder.add_node("node_2", node_2)
      builder.add_node("node_3", node_3)
      builder.add_edge(START, "node_1")
      builder.add_edge("node_1", "node_2")
      builder.add_edge("node_2", "node_3")
      builder.add_edge("node_3", END)
      # æœ€å¾Œä½¿ç”¨ compile() ç”¢ç”Ÿä¸€å€‹ CompiledStateGraph å¯¦ä¾‹
      graph = builder.compile()

      # è¼¸å…¥ä¸€å®šç¬¦åˆ input_schemaï¼Œä½†è¼¸å‡ºç„¡æ³•ä¿è­‰æ˜¯ output_schema
      input_state: InputState = {"user_input": "My"}
      output_state = graph.invoke(
        input_state,
        context={"llm_provider": "openai"},
      )
      # assert output_state == {'graph_output': 'My name is Lance'}

  ```
### state - reducer
+ èªªæ˜
  + <mark>ç”¢å‡ºçµæœçš„èšåˆç­–ç•¥</mark>
  + node å®Œæˆå·¥ä½œæ™‚ï¼Œç”¢å‡ºæˆæœçš„æ¯å€‹ key ä¸ä¸€å®šåªèƒ½ç”¨ overwrite ç­–ç•¥èšåˆåˆ° state ä¸­
  + ä¹Ÿèƒ½é¸ç”¨ addã€mul ç­‰ç­–ç•¥èšåˆåˆ° state ä¸­
  + ä½œç”¨æ™‚é–“ï¼šnode å®Œæˆå·¥ä½œï¼Œä¸¦å›å‚³æˆæœæ™‚
+ ç¯„ä¾‹
  |â˜¢ï¸ <span class="warning">WARNING</span> : BUG|
  |:---|
  | ã€è¼¸å…¥ç‹€æ…‹ã€‘ç«Ÿç„¶è¦å…ˆè·Ÿã€å…¬æœ‰ç‹€æ…‹ã€‘çš„é è¨­å€¼å…ˆ reduce ä¸€æ¬¡ï¼Œè¶…ç´šå¥‡æ€ªï¼<br>è€Œä¸” builtins number types (`int`, `float` ...) é è¨­å€¼èª¤è¢«ä¸€å¾‹ç•¶ä½œ 0ï¼ŒéŒ¯ä¸ŠåŠ éŒ¯ ğŸ¤§ï¼ |
  | workaroundï¼šåœ¨ã€è¼¸å…¥ç‹€æ…‹ã€‘ä½¿ç”¨ `Overwrite` å¥—å€‹è–ç›¾è¡“è·³éç¬¬ä¸€æ¬¡çš„ reduce ç›´æ¥è¦†å¯« |
  ```mermaid
  ---
  config:
    flowchart:
      curve: linear
  ---
  graph TD;
          __start__([<p>__start__</p>]):::first
          add_items(add_items)
          add_more_items(add_more_items)
          finalize(finalize)
          __end__([<p>__end__</p>]):::last
          __start__ --> add_items;
          add_items --> add_more_items;
          add_more_items --> finalize;
          finalize --> __end__;
          classDef default fill:#f2f0ff,line-height:1.2
          classDef first fill-opacity:0
          classDef last fill:#bfb6fc
  ```
  ```py
  import operator
  from typing import Annotated

  from langgraph.graph import END, START, StateGraph
  from langgraph.types import Overwrite
  from typing_extensions import TypedDict


  class OrderState(TypedDict, total=False):
      total_price: float  # reducer: è¦†è“‹ (é è¨­)
      item_count: Annotated[int, operator.add]  # reducer: ç´¯åŠ 
      discount_multiplier: Annotated[float, operator.mul]  # reducer: ç´¯ä¹˜


  def add_items(state: OrderState) -> OrderState:
      # assert state == {'total_price': 0, 'item_count': 0, 'discount_multiplier': 1.0}
      return {
          "item_count": 2,
          "total_price": 200,
          "discount_multiplier": 0.9,
      }


  def add_more_items(state: OrderState) -> OrderState:
      # assert state == {'total_price': 200, 'item_count': 2, 'discount_multiplier': 0.9}
      return {
          "item_count": 1,
          "discount_multiplier": 0.95,
      }


  def finalize(state: OrderState) -> OrderState:
      # assert state == {'total_price': 200, 'item_count': 3, 'discount_multiplier': 0.855}
      return {"total_price": state["total_price"] * state["discount_multiplier"]}  # type: ignore


  if __name__ == "__main__":
      builder = StateGraph(OrderState)
      builder.add_node("add_items", add_items)
      builder.add_node("add_more_items", add_more_items)
      builder.add_node("finalize", finalize)

      builder.add_edge(START, "add_items")
      builder.add_edge("add_items", "add_more_items")
      builder.add_edge("add_more_items", "finalize")
      builder.add_edge("finalize", END)

      graph = builder.compile()

      input_state: OrderState = {
          "item_count": 0,
          "discount_multiplier": Overwrite(1.0),  # type: ignore
          "total_price": 0,
      }

      output_state = graph.invoke(input_state)
      # assert output_state == {'total_price': 171.0, 'item_count': 3, 'discount_multiplier': 0.855}

  ```
+ ç¯„ä¾‹ï¼šparallel
  |â˜¢ï¸ <span class="warning">WARNING</span> : å¤šè·¯åˆä½µ (fan-in) |
  |:---|
  | å‡å¦‚ node Bã€Cã€D æ¥ä¸‹ä¾†éƒ½æµå‘åŒä¸€ nodeï¼Œ<br>ç•¶å®ƒå€‘ä¸¦è¡Œå·¥ä½œæ™‚ï¼Œè‹¥ç”¢å‡ºçµæœçš„ key æ¡ç”¨ overwrite ç­–ç•¥ï¼Œ<br>å‰‡åƒ…èƒ½æœ‰ä¸€å€‹ node å¯«å…¥ã€‚<br>å¦å‰‡é¡¯ç„¶æ˜¯ race conditionï¼Œ<mark>å°‡ç”¢ç”Ÿ `InvalidUpdateError `</mark>ã€‚ |
  | solutionï¼šæŒ‡å®šå¥½ [reducer](#state---reducer)ï¼Œä¸è¦ç”¨ overwrite ç­–ç•¥ |
  ```mermaid
  ---
  config:
    flowchart:
      curve: linear
  ---
  graph TD;
          __start__([<p>__start__</p>]):::first
          A(A)
          B(B)
          C(C)
          D(D)
          __end__([<p>__end__</p>]):::last
          A --> B;
          A --> C;
          A --> D;
          __start__ --> A;
          B --> __end__;
          C --> __end__;
          D --> __end__;
          classDef default fill:#f2f0ff,line-height:1.2
          classDef first fill-opacity:0
          classDef last fill:#bfb6fc
  ```
  ```py
  import asyncio
  import operator
  from typing import Annotated, TypedDict

  from langgraph.graph import END, START, StateGraph


  class MyState(TypedDict):
      log: Annotated[list[str], operator.add]


  async def node_a(state: MyState):
      print("A started")
      await asyncio.sleep(1)
      print("A done")
      return {"log": ["A"]}


  async def node_b(state: MyState):
      print("B started")
      await asyncio.sleep(3)
      print("B done")
      return {"log": ["B"]}


  async def node_c(state: MyState):
      print("C started")
      await asyncio.sleep(2)
      print("C done")
      return {"log": ["C"]}


  async def node_d(state: MyState):
      print("D started")
      await asyncio.sleep(1)
      print("D done")
      return {"log": ["D"]}


  builder = StateGraph(MyState)
  builder.add_node("A", node_a)
  builder.add_node("B", node_b)
  builder.add_node("C", node_c)
  builder.add_node("D", node_d)

  builder.add_edge(START, "A")
  builder.add_edge("A", "B")
  builder.add_edge("A", "C")
  builder.add_edge("A", "D")

  builder.add_edge("B", END)
  builder.add_edge("C", END)
  builder.add_edge("D", END)

  graph = builder.compile()


  async def main():
      result = await graph.ainvoke({"log": []})
      print("\n=== FINAL RESULT ===")
      print(result)
      # {'log': ['A', 'B', 'C', 'D']}


  asyncio.run(main())
  ```

### edge - conditional edge
+ èªªæ˜
  + <mark>æ¢ä»¶å¼è·¯ç”±</mark> (ä¸€å€‹ node æ¥ä¸‹ä¾†å¯æµå‘å¤šå€‹ node æ™‚ï¼Œé¸æ“‡è¦èµ°å“ªé‚Š)
+ ç¯„ä¾‹
  ```mermaid
  ---
  config:
    flowchart:
      curve: linear
  ---
  graph TD
      __start__([__start__])
      judge([judge])
      pass_node([pass_node])
      fail_node([fail_node])
      __end__([__end__])

      __start__ --> judge
      judge -->|score >= 60| pass_node
      judge -->|score < 60| fail_node
      pass_node --> __end__
      fail_node --> __end__

      classDef default fill:#f2f0ff,line-height:1.2
      classDef first fill-opacity:0
      classDef last fill:#bfb6fc

      class __start__ first
      class __end__ last
  ```
  ```py
  import random

  from langgraph.graph import END, START, StateGraph
  from typing_extensions import TypedDict


  class StudentState(TypedDict, total=False):
      name: str
      score: int
      result: str


  if __name__ == "__main__":
      builder = StateGraph(StudentState)
      builder.add_node("judge", lambda state: {"score": random.randint(0, 100)})
      builder.add_node("pass_node", lambda state: {"result": "pass"})
      builder.add_node("fail_node", lambda state: {"result": "fail"})
      builder.add_edge(START, "judge")
      builder.add_conditional_edges("judge", lambda state: "pass_node" if state["score"] >= 60 else "fail_node")
      builder.add_edge("pass_node", END)
      builder.add_edge("fail_node", END)

      graph = builder.compile()
      output_state = graph.invoke({"name": "RogelioKG"})
  ```

### node - send
+ èªªæ˜
  + å¸¸ç”¨æ–¼ <mark>multiple edges</mark> (åŒä¸€ä¸‹æ¸¸ node å…è¨±å¤šç¨® state å¹³è¡ŒåŸ·è¡Œ) 
+ ç¯„ä¾‹
  ```mermaid
  ---
  config:
    flowchart:
      curve: linear
  ---
  graph TD
      __start__([__start__])
      generate_topics([generate_topics])
      generate_joke([generate_joke])
      best_joke([best_joke])
      __end__([__end__])

      __start__ --> generate_topics

      %% Fan-out: subjects = ["lions", "elephants", "penguins"]
      generate_topics -->|Send ...| generate_joke
      generate_topics -->|Send ...| generate_joke
      generate_topics -->|Send ...| generate_joke

      generate_joke --> best_joke
      generate_joke --> best_joke
      generate_joke --> best_joke
      best_joke --> __end__

      classDef default fill:#f2f0ff,line-height:1.2
      classDef first fill-opacity:0
      classDef last fill:#bfb6fc

      class __start__ first
      class __end__ last
  ```
  ```py
  import operator
  from typing import Annotated, TypedDict

  from langgraph.graph import END, START, StateGraph
  from langgraph.types import Send


  class OverallState(TypedDict, total=False):
      topic: str
      subjects: list[str]
      jokes: Annotated[list[str], operator.add]
      best_selected_joke: str


  class PrivateState(TypedDict):
      subject: str


  def generate_topics(state: OverallState) -> OverallState:
      return {"subjects": ["lions", "elephants", "penguins"]}


  def continue_to_jokes(state: OverallState) -> list[Send]:
      return [Send("generate_joke", {"subject": s}) for s in state["subjects"]]  # type: ignore


  def generate_joke(state: PrivateState) -> OverallState:
      joke_map = {
          "lions": "Why don't lions like fast food? Because they can't catch it!",
          "elephants": "Why don't elephants use computers? They're afraid of the mouse!",
          "penguins": "Why don't penguins like talking to strangers at parties? Because they find it hard to break the ice.",
      }
      subject = state["subject"]
      return {"jokes": [joke_map[subject]]}


  def best_joke(state: OverallState) -> OverallState:
      return {"best_selected_joke": "penguins"}


  if __name__ == "__main__":
      builder = StateGraph(OverallState)
      builder.add_node("generate_topics", generate_topics)
      builder.add_node("generate_joke", generate_joke)
      builder.add_node("best_joke", best_joke)
      builder.add_edge(START, "generate_topics")
      builder.add_conditional_edges("generate_topics", continue_to_jokes)
      builder.add_edge("generate_joke", "best_joke")
      builder.add_edge("best_joke", END)
      graph = builder.compile()

      initial_state: OverallState = {"topic": "animals"}
      for step in graph.stream(initial_state):
          print(step)
  ```

### node - command
+ èªªæ˜
  + <mark>åœ¨æ¢ä»¶è·¯ç”±çš„åŒæ™‚ï¼Œæ”¹è®Šå…¬æœ‰ç‹€æ…‹</mark>
+ ç¯„ä¾‹
  ```mermaid
  ---
  config:
    flowchart:
      curve: linear
  ---
  graph TD;
          __start__([<p>__start__</p>]):::first
          init_order(init_order)
          risk_check(risk_check)
          charge_order(charge_order)
          reject_order(reject_order)
          __end__([<p>__end__</p>]):::last
          __start__ --> init_order;
          init_order --> risk_check;
          risk_check -.-> charge_order;
          risk_check -.-> reject_order;
          charge_order --> __end__;
          reject_order --> __end__;
          classDef default fill:#f2f0ff,line-height:1.2
          classDef first fill-opacity:0
          classDef last fill:#bfb6fc

  ```
  ```py
  import operator
  from typing import Annotated, Literal, TypedDict

  from langgraph.graph import END, START, StateGraph
  from langgraph.types import Command


  class OrderState(TypedDict):
      user_id: str
      amount: float
      risk_score: float
      status: str
      logs: Annotated[list[str], operator.add]


  def init_order(state: OrderState) -> dict:
      return {
          "status": "received",
          "logs": [f"æ”¶åˆ°è¨‚å–®ï¼Œé‡‘é¡ = {state['amount']:.2f}"],
      }


  def risk_check(
      state: OrderState,
  ) -> Command[Literal["charge_order", "reject_order"]]:
      amount = state["amount"]

      if amount >= 1000:
          risk_score = 0.9
          status = "rejected_high_risk"
          goto = "reject_order"

          log_msg = f"é¢¨éšªå¯©æ ¸ï¼šé‡‘é¡ {amount:.2f}ï¼Œåˆ¤å®šç‚ºé«˜é¢¨éšªï¼ˆrisk={risk_score}ï¼‰ï¼Œå°‡æ‹’çµ•æ­¤è¨‚å–®ã€‚"

      else:
          risk_score = 0.1
          status = "approved"
          goto = "charge_order"

          log_msg = f"é¢¨éšªå¯©æ ¸ï¼šé‡‘é¡ {amount:.2f}ï¼Œåˆ¤å®šç‚ºä½é¢¨éšªï¼ˆrisk={risk_score}ï¼‰ï¼Œå‡†è¨±é€²å…¥æ‰£æ¬¾ã€‚"

      return Command(
          update={
              "risk_score": risk_score,
              "status": status,
              "logs": [log_msg],
          },
          goto=goto,
      )


  def charge_order(state: OrderState) -> dict:
      amount = state["amount"]
      log_msg = f"æ‰£æ¬¾æˆåŠŸï¼šå·²å‘ä½¿ç”¨è€… {state['user_id']} æ”¶è²» {amount:.2f} å…ƒã€‚"

      return {
          "status": "charged",
          "logs": [log_msg],
      }


  def reject_order(state: OrderState) -> dict:
      log_msg = f"è¨‚å–®å·²è¢«æ‹’çµ•ï¼Œæœ€çµ‚ç‹€æ…‹ = {state['status']}ã€‚"
      return {
          "logs": [log_msg],
      }


  if __name__ == "__main__":
      builder = StateGraph(OrderState)
      builder.add_node("init_order", init_order)
      builder.add_node("risk_check", risk_check)
      builder.add_node("charge_order", charge_order)
      builder.add_node("reject_order", reject_order)
      builder.add_edge(START, "init_order")
      builder.add_edge("init_order", "risk_check")
      builder.add_edge("charge_order", END)
      builder.add_edge("reject_order", END)
      graph = builder.compile()

      low_amount_state: OrderState = {
          "user_id": "user_1",
          "amount": 199.0,
          "risk_score": 0.0,
          "status": "",
          "logs": [],
      }
      result_low = graph.invoke(low_amount_state)
      print(result_low)

      high_amount_state: OrderState = {
          "user_id": "user_2",
          "amount": 2000.0,
          "risk_score": 0.0,
          "status": "",
          "logs": [],
      }
      result_high = graph.invoke(high_amount_state)
      print(result_high)

  ```

### graph - subgraph

+ ç¯„ä¾‹ï¼šå¯«æ³•ä¸€
  > å¼•ç”¨ global çš„ subgraphï¼Œç›´æ¥å…§åµŒé€² parent graph çš„ node æµç¨‹è£¡
  ```py
  def call_subgraph(state: State):
      subgraph_output = subgraph.invoke({"bar": state["foo"]})
      return {"foo": subgraph_output["bar"]}
  ```
+ ç¯„ä¾‹ï¼šå¯«æ³•äºŒ
  > å°‡ subgraph è¦–ä½œä¸€å€‹ parent graph çš„ node
  ```py
  builder.add_node("node_1", subgraph)
  ```

### grpah - detailed info
+ é™„åœ–
  ![](https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/checkpoints.jpg?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=966566aaae853ed4d240c2d0d067467c)
+ èªªæ˜
  + graphï¼šå·¥ä½œæµåœ–
  + super-stepï¼šå¤§æ­¥é©Ÿ
    + ç•¶ super-step N çš„æ¯å€‹ node éƒ½å®Œæˆå¾Œï¼Œæ‰å‘å‰æ¨é€²è‡³ super-step N+1
  + checkpointï¼šæª¢æŸ¥é»
    + åœ¨æ¯å€‹ super-step çµæŸæ™‚ï¼Œç”¢ç”Ÿæª¢æŸ¥é»ï¼Œä»¥ `StateSnapshot` çµæ§‹è¡¨ç¤º
    + è‹¥ super-step N ç™¼ç”Ÿéƒ¨åˆ† node å¤±æ•—
      + å·²æˆåŠŸçš„ node å¯«å…¥ pending writes
      + retry æ™‚åªé‡æ–°è·‘å¤±æ•— node
  + threadï¼šä¸€æ¬¡å®Œæ•´å·¥ä½œæµçš„æ‰€æœ‰æª¢æŸ¥é»


### graph - BSP
+ èªªæ˜
  > Langgraph çš„å¹³è¡Œè¨ˆç®—æ¨¡å‹æ¡ç”¨çš„æ˜¯ [Bulk Synchronous Parallel](https://en.wikipedia.org/wiki/Bulk_synchronous_parallel) (BSP)ï¼Œ\
  > åœ¨æ­¤åŸºç¤ä¸Šï¼Œä¸€å€‹ graph å¹³è¡Œè¨ˆç®—æœƒè¢«åˆ†æˆå¤šå€‹ super-stepï¼Œ\
  > ç•¶ super-step N çš„æ¯å€‹ node éƒ½å®Œæˆå¾Œï¼Œæ‰å‘å‰æ¨é€²è‡³ super-step N+1ã€‚
  > å¦‚ä¸‹ç¯„ä¾‹ï¼š
  > 
  > + super-step 0: A
  > + super-step 1: B
  > + super-step 2: C, F
  > + super-step 3: D, G, I
  > + super-step 4: K, H, J
  > + super-step 5: L
  > + super-step 6: E
  > 
  > å‡è¨­<mark>F é˜»å¡ï¼Œå³ä¾¿ C å·²ç¶“å®Œæˆå·¥ä½œï¼Œå®ƒé‚„æ˜¯æœƒè€å¯¦ç­‰ F å®Œæˆ</mark>ã€‚\
  > ä¹‹æ‰€ä»¥è¦è¨­ç«‹é€™ç¨®é™åˆ¶ï¼Œæ˜¯ç‚ºäº†<mark>è®“ graph å¹³è¡Œè¨ˆç®—æ˜¯ deterministic</mark>ï¼Œ\
  > ä¸€æ—¦ deterministicï¼Œ<mark>æˆ‘å€‘å°±æœ‰åè¶³ä¿¡å¿ƒ rollback è€Œå¾Œ replay</mark>ã€‚

+ ç¯„ä¾‹
  |â˜¢ï¸ <span class="warning">WARNING</span> : å¤šè·¯åˆä½µ (fan-in) |
  |:---|
  | Langgraph è‹¥ä½¿ç”¨åŒæ­¥å¯«æ³•ï¼Œæœƒ<mark>è‡ªå‹•æ¡ç”¨ multithreading</mark> |
  ```mermaid
  ---
  config:
    flowchart:
      curve: linear
  ---
  graph LR;
          __start__([<p>__start__</p>]):::first
          A(A)
          B(B)
          C(C)
          D(D)
          K(K)
          E(E)
          F(F)
          G(G)
          H(H)
          I(I)
          J(J)
          L(L)
          __end__([<p>__end__</p>]):::last
          A --> B;
          B --> C;
          B --> F;
          C --> D;
          D --> K;
          F --> G;
          F --> I;
          G --> H;
          G --> J;
          H --> E;
          I --> J;
          J --> L;
          K --> E;
          L --> E;
          __start__ --> A;
          E --> __end__;
          classDef default fill:#f2f0ff,line-height:1.2
          classDef first fill-opacity:0
          classDef last fill:#bfb6fc
  ```
  ```py
  import asyncio
  import operator
  import time
  from typing import Annotated

  from langgraph.checkpoint.memory import InMemorySaver
  from langgraph.graph import END, START, StateGraph
  from typing_extensions import TypedDict


  class State(TypedDict):
      log: Annotated[list[str], operator.add]


  def now() -> str:
      # return f"{time.time():.3f}"
      return ""


  def make_node(name: str, delay: float):
      async def _node(state: State) -> dict:
          print(f"[{now()}] {name} START (sleep {delay}s)")
          await asyncio.sleep(delay)
          print(f"[{now()}] {name} END")
          return {"log": [name]}

      _node.__name__ = f"node_{name}"
      return _node


  def create_workflow():
      delays = {
          "A": 0.4,
          "B": 0.5,
          "C": 0.2,
          "D": 0.3,
          "K": 0.7,
          "E": 0.2,
          "F": 10.0,  # æ•…æ„è¨­å¾ˆæ…¢ â†’ ç”¨ä¾†è§€å¯Ÿ BSP barrier
          "G": 0.5,
          "H": 0.4,
          "I": 0.8,
          "J": 0.3,
          "L": 0.2,
      }

      builder = StateGraph(State)

      for name, d in delays.items():
          builder.add_node(name, make_node(name, d))

      builder.add_edge(START, "A")
      builder.add_edge("A", "B")
      builder.add_edge("B", "C")
      builder.add_edge("C", "D")
      builder.add_edge("D", "K")
      builder.add_edge("K", "E")
      builder.add_edge("B", "F")
      builder.add_edge("F", "G")
      builder.add_edge("G", "H")
      builder.add_edge("H", "E")
      builder.add_edge("F", "I")
      builder.add_edge("I", "J")
      builder.add_edge("G", "J")
      builder.add_edge("J", "L")
      builder.add_edge("L", "E")
      builder.add_edge("E", END)

      graph = builder.compile()

      return graph


  async def main():
      graph = create_workflow()
      print(graph.get_graph().draw_mermaid())
      result = await graph.ainvoke({"log": []})
      print(result)


  if __name__ == "__main__":
      asyncio.run(main())

  ```

## [Functional API](https://docs.langchain.com/oss/python/langgraph/functional-api)



## Persistence

### replay
+ èªªæ˜
  + <mark>Moody Blue!</mark>
    ![](https://i.redd.it/hyl211d35jjb1.jpg)
  + è‹¥éœ€è¦å…ˆè®Šæ›´ state å†é‡æ’­
    + è«‹å…ˆç²å–æ–° config
      ```py
      new_config = graph.update_state(replay_config, values=new_state)
      ```
    + å†æ‹¿å» invoke
      ```py
      graph.invoke(None, new_config)
      ```
  + è‹¥ä¸éœ€è¦è®Šæ›´ state ç›´æ¥é‡æ’­
    + ç›´æ¥ invoke
      ```py
      graph.invoke(None, replay_config)
      ```
+ ç¯„ä¾‹
  ```py
  from langchain_core.runnables.config import RunnableConfig
  from langgraph.checkpoint.memory import InMemorySaver
  from langgraph.graph import END, START, StateGraph
  from typing_extensions import TypedDict


  class State(TypedDict):
      base: int
      user_input: int


  def node_a(state: State):
      print(state)
      print("run A")
      return {"base": state["base"] + 1}


  def node_b(state: State):
      print(state)
      print("run B")
      return {"base": state["base"] + 10}


  def node_c(state: State):
      print(state)
      print("run C (divide)")
      return {"base": state["base"] / state["user_input"]}


  def create_workflow():
      builder = StateGraph(State)

      builder.add_node("A", node_a)
      builder.add_node("B", node_b)
      builder.add_node("C", node_c)

      builder.add_edge(START, "A")
      builder.add_edge("A", "B")
      builder.add_edge("B", "C")
      builder.add_edge("C", END)

      cp = InMemorySaver()
      graph = builder.compile(checkpointer=cp)
      return graph


  if __name__ == "__main__":
      graph = create_workflow()
      config: RunnableConfig = {"configurable": {"thread_id": "t1"}}

      print("\n===== FIRST RUN =====")
      # âš ï¸ ç¬¬ä¸€æ¬¡æˆ‘å€‘æ•…æ„é™¤ä»¥ 0ï¼Œè®“å®ƒå‡ºéŒ¯
      try:
          graph.invoke({"base": 1, "user_input": 0}, config)
      except Exception as e:
          print("Graph stopped due to error:", e)

      history = list(graph.get_state_history(config))

      print("\n===== CHECKPOINT HISTORY =====")
      last_good_snapshot = None
      for h in history:
          cid = h.config["configurable"]["checkpoint_id"]  # type: ignore
          step = h.metadata["step"]  # type: ignore
          err = [t.error for t in h.tasks]
          print(f"checkpoint={cid}, step={step}, error={err}")
          if not any(err):
              last_good_snapshot = h
              break

      assert last_good_snapshot is not None
      replay_config = last_good_snapshot.config

      print("\n===== SECOND RUN =====")
      # âœ… æœƒ branch å‡ºä¸€å€‹æ–°çš„ checkpointï¼Œå†å¾é€™è£¡é–‹å§‹åŸ·è¡Œ
      new_config = graph.update_state(replay_config, values={"user_input": 5})
      result = graph.invoke(None, new_config)

      print("\n===== FINAL RESULT =====")
      print(result)
  ```

### short-term memory
+ èªªæ˜
  + çŸ­æœŸè¨˜æ†¶ï¼Œä½¿ç”¨<mark>è·¨ thread çš„ keyâ€“value database</mark>
+ ç¯„ä¾‹
  ```py
  # 1. å»ºç«‹ Store
  from langgraph.store.memory import InMemoryStore

  store = InMemoryStore()
  ```
  ```py
  # 2. æŒ‡å®š namespaceï¼ˆé€šå¸¸ç”¨ user_idï¼‰
  namespace = ("1", "memories")
  ```
  ```py
  # 3. å¯«å…¥è¨˜æ†¶
  store.put(namespace, "id-1", {"food": "pizza"})
  store.put(namespace, "id-2", {"color": "blue"})
  ```
  ```py
  # 4. æœå°‹è¨˜æ†¶
  memories = store.search(namespace)
  ```
  ```py
  # 5. è¨­å®šèªç¾©æœå°‹
  store = InMemoryStore(
      index={
          "embed": init_embeddings("openai:text-embedding-3-small"),
          "dims": 1536,
          "fields": ["$"]
      }
  )
  store.search(namespace, query="what food does the user like?") # âœ… ä½¿ç”¨è‡ªç„¶èªè¨€ï¼
  ```

### long-term memory
+ èªªæ˜
  + é•·æœŸè¨˜æ†¶ï¼Œä½¿ç”¨<mark>è·¨ thread çš„ keyâ€“value database</mark>
+ ç¯„ä¾‹
  ```py
  import uuid
  from typing import Any, Literal, cast

  from dotenv import load_dotenv
  from langchain.chat_models import init_chat_model
  from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
  from langchain_core.runnables import RunnableConfig
  from langgraph.graph import START, MessagesState, StateGraph
  from langgraph.store.base import BaseStore
  from langgraph.store.memory import InMemoryStore

  load_dotenv()

  store = InMemoryStore()  # âœ… é•·æœŸè¨˜æ†¶
  base_model = init_chat_model("gpt-4o-mini")
  chat_model = base_model.bind(temperature=0.7)
  extract_model = base_model.bind(max_tokens=64, temperature=0)
  memory_intent_model = base_model.bind(max_tokens=256, temperature=0)

  type Intent = Literal["store", "normal"]


  def classify_memory_intent(user_msg: str) -> Intent:
      prompt = f"""
      ä½ æ˜¯ä¸€å€‹ memory-intent classifierã€‚
      å°‡ä½¿ç”¨è€…è¨Šæ¯åˆ†é¡ç‚ºä»¥ä¸‹å…©é¡ä¹‹ä¸€ï¼š

      1. store   â†’ ä½¿ç”¨è€…è¦æ±‚ä½ è¨˜ä½æŸä»¶äº‹
      3. normal  â†’ å…¶ä»–èŠå¤©è¨Šæ¯

      åƒ…å›ç­”ä»¥ä¸‹å…©å€‹å­—ä¹‹ä¸€ï¼šstore / normal

      ä½¿ç”¨è€…è¨Šæ¯ï¼š
      {user_msg}
      """

      ai_response = cast(str, memory_intent_model.invoke(prompt).content).strip().lower()
      return "store" if ai_response.startswith("store") else "normal"


  def extract_memory(user_msg: str) -> dict[str, Any]:
      prompt = f"""
      è«‹å¾ä½¿ç”¨è€…è¨Šæ¯ä¸­æŠ½å–ã€Œè¦è¨˜ä½çš„è³‡è¨Šã€ï¼Œçµ±ä¸€è¼¸å‡ºæ ¼å¼å¦‚ä¸‹ï¼š
      {{
          "key": "è¨˜æ†¶ä¸»æ—¨",
          "value": "å…·é«”å…§å®¹"
      }}
      ä½¿ç”¨è€…è¨Šæ¯ï¼š
      {user_msg}
      """

      content = cast(str, extract_model.invoke(prompt).content)

      try:
          import json

          return json.loads(content)
      except Exception:
          return {"key": str(uuid.uuid4()), "value": content}


  def chat_node(state: MessagesState, config: RunnableConfig, *, store: BaseStore):
      user_id = config["configurable"]["user_id"]  # type: ignore
      namespace = ("users", user_id)  # ä½¿ç”¨ user id ç•¶ä½œè¨˜æ†¶ namespace
      user_msg = cast(str, state["messages"][-1].content)

      match classify_memory_intent(user_msg):
          case "store":
              mem = extract_memory(user_msg)
              store.put(namespace, mem["key"], mem["value"])
              return {"messages": [AIMessage(content=f"æˆ‘å·²ç¶“è¨˜ä½äº†ï¼\nè¨˜éŒ„å…§å®¹ï¼š{mem}")]}

          case "normal":
              items = store.search(namespace, query="*", limit=40)  # èŠå¤©å¸¶è‘—ä¸€äº›è¨˜æ†¶ä¸Šè·¯
              memories = "\n".join(str(i.value) for i in items)
              response = chat_model.invoke(
                  [
                      SystemMessage(
                          f"ä½ æ˜¯ä¸€å€‹æ“æœ‰é•·æœŸè¨˜æ†¶èƒ½åŠ›çš„åŠ©æ‰‹ï¼Œè«‹ä½ æ ¹æ“šè¨˜ä½çš„å…§å®¹ï¼Œä»¥æ–‡å­—å›è¦†ä»–ã€‚\n=== è¨˜æ†¶ ===\n{memories}"
                      )
                  ]
                  + state["messages"]
              )
              return {"messages": [response]}


  def create_workflow():
      builder = StateGraph(MessagesState)
      builder.add_node(chat_node)
      builder.add_edge(START, "chat_node")
      graph = builder.compile(store=store)
      return graph


  if __name__ == "__main__":
      graph = create_workflow()
      cfg: RunnableConfig = {"configurable": {"thread_id": "t1", "user_id": "u1"}}

      tests = [
          "å—¨ï¼ä½ å¯ä»¥å¹«æˆ‘å®‰æ’ä¸€å€‹å¥èº«è¨ˆç•«å—ï¼Ÿæˆ‘æ¥ä¸‹ä¾†æœƒè©³ç´°èªªæ˜æœ‰å“ªäº›æ³¨æ„äº‹é …ã€‚",
          "è¨˜ä½ï¼šæˆ‘çš„å¥èº«ç›®æ¨™æ˜¯å¢è‚Œï¼Œå°¤å…¶æ˜¯ä¸ŠåŠèº«ã€‚",
          "è¨˜ä½ï¼šæˆ‘å·¦è‚©æœ‰èˆŠå‚·ï¼Œä¸èƒ½åšéé ­æ¨èˆ‰ã€‚",
          "è¨˜ä½ï¼šæˆ‘åå¥½ä½¿ç”¨å•éˆ´ã€ä¸å–œæ­¡å™¨æ¢°ã€‚",
          "è¨˜ä½ï¼šæˆ‘æ¯é€±å››å¤©å¯è¨“ç·´ï¼šäºŒã€å››ã€å…­ã€æ—¥ã€‚",
          "ä¸‹æ¬¡è¨“ç·´ï¼Œæˆ‘æƒ³åšéé ­æ¨èˆ‰ï¼Œä½ è¦ºå¾—å¯ä»¥å—ï¼Ÿè«‹ä¾æˆ‘çš„é™åˆ¶èˆ‡åå¥½çµ¦äºˆå»ºè­°ã€‚",
          "ä»Šå¤©æ˜¯ç¦®æ‹œäº”ï¼Œè«‹ä¾æˆ‘çš„é™åˆ¶èˆ‡åå¥½ï¼Œå®‰æ’ä»Šå¤©çš„è¨“ç·´å…§å®¹ã€‚",
      ]

      for t in tests:
          result = graph.invoke({"messages": [HumanMessage(t)]}, cfg)
          print("ğŸ‘¤:", t)
          print("ğŸ¤–:", result["messages"][-1].content)
          print("-" * 40)
          # ğŸ‘¤: å—¨ï¼ä½ å¯ä»¥å¹«æˆ‘å®‰æ’ä¸€å€‹å¥èº«è¨ˆç•«å—ï¼Ÿæˆ‘æ¥ä¸‹ä¾†æœƒè©³ç´°èªªæ˜æœ‰å“ªäº›æ³¨æ„äº‹é …ã€‚
          # ğŸ¤–: å—¨ï¼ç•¶ç„¶å¯ä»¥ï¼Œè«‹å‘Šè¨´æˆ‘ä½ çš„æ³¨æ„äº‹é …å’Œä»»ä½•å…·é«”çš„ç›®æ¨™æˆ–éœ€æ±‚ï¼Œæˆ‘æœƒæ ¹æ“šé€™äº›è³‡è¨Šä¾†å¹«ä½ å®‰æ’ä¸€å€‹å¥èº«è¨ˆç•«ã€‚
          # ----------------------------------------
          # ğŸ‘¤: è¨˜ä½ï¼šæˆ‘çš„å¥èº«ç›®æ¨™æ˜¯å¢è‚Œï¼Œå°¤å…¶æ˜¯ä¸ŠåŠèº«ã€‚
          # ğŸ¤–: æˆ‘å·²ç¶“è¨˜ä½äº†ï¼
          # è¨˜éŒ„å…§å®¹ï¼š{'key': 'å¥èº«ç›®æ¨™', 'value': 'å¢è‚Œï¼Œå°¤å…¶æ˜¯ä¸ŠåŠèº«'}
          # ----------------------------------------
          # ğŸ‘¤: è¨˜ä½ï¼šæˆ‘å·¦è‚©æœ‰èˆŠå‚·ï¼Œä¸èƒ½åšéé ­æ¨èˆ‰ã€‚
          # ğŸ¤–: æˆ‘å·²ç¶“è¨˜ä½äº†ï¼
          # è¨˜éŒ„å…§å®¹ï¼š{'key': 'å·¦è‚©èˆŠå‚·', 'value': 'ä¸èƒ½åšéé ­æ¨èˆ‰'}
          # ----------------------------------------
          # ğŸ‘¤: è¨˜ä½ï¼šæˆ‘åå¥½ä½¿ç”¨å•éˆ´ã€ä¸å–œæ­¡å™¨æ¢°ã€‚
          # ğŸ¤–: æˆ‘å·²ç¶“è¨˜ä½äº†ï¼
          # è¨˜éŒ„å…§å®¹ï¼š{'key': 'å¥èº«å™¨æåå¥½', 'value': 'åå¥½ä½¿ç”¨å•éˆ´ï¼Œä¸å–œæ­¡å™¨æ¢°'}
          # ----------------------------------------
          # ğŸ‘¤: è¨˜ä½ï¼šæˆ‘æ¯é€±å››å¤©å¯è¨“ç·´ï¼šäºŒã€å››ã€å…­ã€æ—¥ã€‚
          # ğŸ¤–: æˆ‘å·²ç¶“è¨˜ä½äº†ï¼
          # è¨˜éŒ„å…§å®¹ï¼š{'key': 'è¨“ç·´æ—¥', 'value': 'æ¯é€±å››å¤©å¯è¨“ç·´ï¼šäºŒã€å››ã€å…­ã€æ—¥'}
          # ----------------------------------------
          # ğŸ‘¤: ä¸‹æ¬¡è¨“ç·´ï¼Œæˆ‘æƒ³åšéé ­æ¨èˆ‰ï¼Œä½ è¦ºå¾—å¯ä»¥å—ï¼Ÿè«‹ä¾æˆ‘çš„é™åˆ¶èˆ‡åå¥½çµ¦äºˆå»ºè­°ã€‚
          # ğŸ¤–: æ ¹æ“šä½ çš„é™åˆ¶ï¼Œéé ­æ¨èˆ‰æ˜¯ä¸å»ºè­°çš„ã€‚å¦‚æœä½ æƒ³å¢è‚Œä¸ŠåŠèº«ï¼Œå¯ä»¥è€ƒæ…®ä½¿ç”¨å•éˆ´é€²è¡Œå…¶ä»–ä¸ŠåŠèº«çš„è¨“ç·´ï¼Œä¾‹å¦‚å•éˆ´è‚©æ¨ã€å•éˆ´è‡¥æ¨æˆ–å•éˆ´åˆ’èˆ¹ç­‰ã€‚é€™äº›å‹•ä½œå¯ä»¥å¹«åŠ©ä½ é”åˆ°å¢è‚Œçš„ç›®æ¨™ï¼ŒåŒæ™‚é¿å…åšéé ­æ¨èˆ‰çš„é¢¨éšªã€‚ä½ å¯ä»¥æ ¹æ“šæ¯é€±çš„è¨“ç·´è¨ˆåŠƒï¼Œå®‰æ’é€™äº›å‹•ä½œåœ¨äºŒã€å››ã€å…­æˆ–æ—¥çš„è¨“ç·´ä¸­ã€‚
          # ----------------------------------------
          # å¢è‚Œï¼Œå°¤å…¶æ˜¯ä¸ŠåŠèº«
          # ä¸èƒ½åšéé ­æ¨èˆ‰
          # åå¥½ä½¿ç”¨å•éˆ´ï¼Œä¸å–œæ­¡å™¨æ¢°
          # æ¯é€±å››å¤©å¯è¨“ç·´ï¼šäºŒã€å››ã€å…­ã€æ—¥
          # ğŸ‘¤: ä»Šå¤©æ˜¯ç¦®æ‹œäº”ï¼Œè«‹ä¾æˆ‘çš„é™åˆ¶èˆ‡åå¥½ï¼Œå®‰æ’ä»Šå¤©çš„è¨“ç·´å…§å®¹ã€‚
          # ğŸ¤–: ä»Šå¤©æ˜¯ç¦®æ‹œäº”ï¼Œæ ¹æ“šä½ çš„è¨“ç·´å®‰æ’ï¼Œé€™æ˜¯ä¼‘æ¯æ—¥ã€‚å»ºè­°ä½ å¯ä»¥åˆ©ç”¨é€™ä¸€å¤©é€²è¡Œè¼•é¬†çš„æ´»å‹•ï¼Œå¦‚ä¼¸å±•ã€ç‘œä¼½æˆ–è¼•é¬†æ•£æ­¥ï¼Œä¾†ä¿ƒé€²æ¢å¾©ã€‚æ˜å¤©ï¼ˆæ˜ŸæœŸå…­ï¼‰å¯ä»¥ç¹¼çºŒé€²è¡Œå¢è‚Œè¨“ç·´ï¼Œç‰¹åˆ¥æ³¨æ„ä¸ŠåŠèº«çš„è¨“ç·´ã€‚å¦‚æœä½ éœ€è¦ï¼Œæˆ‘å¯ä»¥å¹«ä½ è¨ˆåŠƒæ˜å¤©çš„è¨“ç·´å…§å®¹ï¼

  ```

### library
+ æä¾› checkpoint æŒä¹…åŒ–çš„å‡½å¼åº«
  + `langgraph-checkpoint`ï¼šIn-memory (builtins)
  + `langgraph-checkpoint-sqlite`ï¼šSQLite (3rd lib)
  + `langgraph-checkpoint-postgres`ï¼šPostgres (3rd lib)
  + `langgraph-checkpoint-redis`ï¼šRedis (3rd lib)

## Durable Execution

### task
+ èªªæ˜
  + <mark>å°‡ non-deterministic çš„æ“ä½œåŒ…æˆ taskï¼Œä»¥ä¾¿åœ¨ replay æ™‚ï¼Œèƒ½å¤ é‡ç”¨ task çµæœ</mark>
+ ç¯„ä¾‹
  ```py
  import random
  import uuid
  from typing import TypedDict

  from langchain_core.runnables.config import RunnableConfig
  from langgraph.checkpoint.memory import InMemorySaver
  from langgraph.func import task
  from langgraph.graph import END, START, StateGraph
  from langgraph.types import CachePolicy

  FLAG = 0


  class State(TypedDict):
      value: int


  @task(cache_policy=CachePolicy(key_func=lambda x: str(x)))
  def random_task(x: int) -> int:
      r = random.randint(1, 1000000)
      print(f"[RUN random_task] x={x}, random={r}")
      return r


  def node_main(state: State) -> State:
      fut = random_task(state["value"])
      result = fut.result()
      print(f"node_main finished task: result={result}")
      global FLAG
      if FLAG == 0:
          FLAG += 1
          raise Exception("Bruh!")
      else:
          return {"value": result}


  def node_end(state: State):
      print("Reached END")
      return {}


  def create_workflow():
      builder = StateGraph(State)
      builder.add_node("main", node_main)
      builder.add_node("end", node_end)

      builder.add_edge(START, "main")
      builder.add_edge("main", "end")
      builder.add_edge("end", END)

      graph = builder.compile(
          checkpointer=InMemorySaver(),
      )
      return graph


  if __name__ == "__main__":
      graph = create_workflow()
      thread_id = uuid.uuid4()

      config: RunnableConfig = {
          "configurable": {"thread_id": thread_id},
      }

      print("========== RUN #1 ==========")
      try:
          graph.invoke({"value": 42}, config=config)
      except BaseException as e:
          print(f"[INTERRUPTED]: {e}")

      print("\n========== RUN #2 (replay) ==========")
      out = graph.invoke(None, config=config)
      print("Result:", out)

      # ========== RUN #1 ==========
      # [RUN random_task] x=42, random=873769
      # node_main finished task: result=873769
      # [INTERRUPTED]: Bruh!

      # ========== RUN #2 (replay) ==========
      # node_main finished task: result=873769
      # Reached END
      # Result: {'value': 873769}

  ```

### modes
+ ç¨®é¡
  + `exit`ï¼šä¸€æ¬¡å·¥ä½œæµå®Œæˆæ™‚ï¼Œæ‰å„²å­˜æ‰€æœ‰ checkpoints
  + `async`ï¼šæ¯å€‹ super-step éƒ½æœƒå¯«å…¥ checkpointï¼Œæ¡ç”¨ã€Œç•°æ­¥ã€å¯«å…¥ (âœ… é è¨­)
  + `sync`ï¼šæ¯å€‹ super-step éƒ½æœƒå¯«å…¥ checkpointï¼Œæ¡ç”¨ã€ŒåŒæ­¥ã€å¯«å…¥
+ ç¯„ä¾‹
  ```py
  graph.invoke(
      ...,
      durability="sync"
  )
  ```


### cache

```py
import random
import uuid
from typing import TypedDict

from langchain_core.runnables.config import RunnableConfig
from langgraph.cache.memory import InMemoryCache
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.func import task
from langgraph.graph import END, START, StateGraph
from langgraph.types import CachePolicy


# cache key: ä»¥è¼¸å…¥æ•¸å­—ç•¶ key
@task(cache_policy=CachePolicy(key_func=lambda x: str(x)))
def random_task(x: int) -> int:
    r = random.randint(1, 10)
    print(f"[RUN random_task] x={x}, random={r}")
    return r


class State(TypedDict):
    value: int


def node_main(state: State):
    fut = random_task(state["value"])
    result = fut.result()
    return {"value": result}


def create_workflow():
    builder = StateGraph(State)
    builder.add_node("main", node_main)
    builder.add_edge(START, "main")
    builder.add_edge("main", END)

    # âœ… åªè¦é€™è£¡å¤šçµ¦å®šä¸€å€‹ cache option
    graph = builder.compile(checkpointer=InMemorySaver(), cache=InMemoryCache())
    return graph


if __name__ == "__main__":
    graph = create_workflow()
    thread_id = uuid.uuid4()
    config: RunnableConfig = {"configurable": {"thread_id": thread_id}}
    print("---- RUN #1 ----")
    out1 = graph.invoke({"value": 42}, config=config)
    print("Result:", out1) # Result: {'value': 9}
    print("\n---- RUN #2 (cache) ----")
    out2 = graph.invoke({"value": 42}, config=config)
    print("Result:", out2) # Result: {'value': 9}
```


## Streaming

### chat model interface
+ èªªæ˜
  + <mark>ä¸€å€‹çµ±ä¸€çš„èŠå¤©æ¨¡å‹ä»‹é¢</mark>
  + ç„¡è«–æ˜¯å„å®¶å¤§å» çš„ LLM æ¨¡å‹ï¼Œé‚„æ˜¯è‡ªå·±é–‹ç™¼çš„ LLM æ¨¡å‹ï¼Œéƒ½å¯ä»¥ä»¥é€™å¥—ä»‹é¢æ¥å…¥ langchain ç”Ÿæ…‹
+ ç¯„ä¾‹
  ```py
  import asyncio
  import time
  from collections.abc import AsyncIterator, Iterator
  from typing import Any

  from langchain_core.language_models.chat_models import BaseChatModel
  from langchain_core.messages import (
      AIMessage,
      AIMessageChunk,
      BaseMessage,
  )
  from langchain_core.outputs import (
      ChatGeneration,
      ChatGenerationChunk,
      ChatResult,
  )

  # âœ… invoke çš„ input å¯ä»¥å‚³å…¥ str | list[str]
  # âœ… str æœƒè¢«åŒ…è£æˆ Messageï¼Œå› æ­¤ messages æ¨™è¨»ç‚º list[BaseMessage]

  class DummyChatModel(BaseChatModel):
      @property
      def _llm_type(self) -> str:
          return "dummy"

      @property
      def _identifying_params(self) -> dict[str, Any]:
          return {}

      def _generate(
          self,
          messages: list[BaseMessage],
          stop: list[str] | None = None,
          **kwargs: Any,
      ) -> ChatResult:
          """åŒæ­¥ä¸€æ¬¡ç”Ÿæˆ"""
          last_content = " ".join(str(message.content) for message in messages)
          full_text = f"[echo] {last_content}"
          ai_msg = AIMessage(content=full_text)
          gen = ChatGeneration(message=ai_msg)
          return ChatResult(generations=[gen])

      async def _agenerate(
          self,
          messages: list[BaseMessage],
          stop: list[str] | None = None,
          **kwargs: Any,
      ) -> ChatResult:
          """ç•°æ­¥ä¸€æ¬¡ç”Ÿæˆ"""
          last_content = " ".join(str(message.content) for message in messages)
          full_text = f"[echo] {last_content}"
          ai_msg = AIMessage(content=full_text)
          gen = ChatGeneration(message=ai_msg)
          return ChatResult(generations=[gen])

      def _stream(
          self,
          messages: list[BaseMessage],
          stop: list[str] | None = None,
          **kwargs: Any,
      ) -> Iterator[ChatGenerationChunk]:
          """åŒæ­¥ä¸²æµç”Ÿæˆ"""
          last_content = " ".join(str(message.content) for message in messages)
          full_text = f"[echo] {last_content}"
          for token in full_text.split():
              chunk = AIMessageChunk(content=token + "~")
              yield ChatGenerationChunk(message=chunk)
              time.sleep(1)  # æ¨¡æ“¬ token ç”¢å‡ºå»¶é²

      async def _astream(
          self,
          messages: list[BaseMessage],
          stop: list[str] | None = None,
          **kwargs: Any,
      ) -> AsyncIterator[ChatGenerationChunk]:
          """ç•°æ­¥ä¸²æµç”Ÿæˆ"""
          last_content = " ".join(str(message.content) for message in messages)
          full_text = f"[echo] {last_content}"
          for token in full_text.split():
              chunk = AIMessageChunk(content=token + "~")
              yield ChatGenerationChunk(message=chunk)
              await asyncio.sleep(1)  # æ¨¡æ“¬ token ç”¢å‡ºå»¶é²


  if __name__ == "__main__":
      import asyncio

      model = DummyChatModel()

      print("=== sync invoke ===")
      print(model.invoke(["Hello sync", "Hello world!"]).content)
      # === sync invoke ===
      # [echo] Hello sync Hello world!

      print("\n=== sync stream ===")
      for chunk in model.stream("Hello sync streaming"):
          print("â†’", chunk.content)
          # === async astream ===
          # â†’ [echo]~
          # â†’ Hello~
          # â†’ async~
          # â†’ streaming~
          # â†’

      async def test_async():
          print("\n=== async ainvoke ===")
          resp = await model.ainvoke("Hello async")
          print(resp.content)
          # === async astream ===
          # â†’ [echo]~
          # â†’ Hello~
          # â†’ async~
          # â†’ streaming~
          # â†’

          print("\n=== async astream ===")
          async for chunk in model.astream("Hello async streaming"):
              print("â†’", chunk.content)
              # === async astream ===
              # â†’ [echo]~
              # â†’ Hello~
              # â†’ async~
              # â†’ streaming~
              # â†’

      asyncio.run(test_async())
  ```



### modes
+ ç¨®é¡
  + `updates`ï¼š<mark>æ¯å€‹ node çš„ state æ›´æ–°ç‰‡æ®µ</mark>
    ```py
    {'refine_topic': {'topic': 'ice cream and cats'}}
    {'generate_joke': {'joke': 'This is a joke about ice cream and cats'}}
    ```
  + `values`ï¼š<mark>æ¯æ¬¡ super-step çš„ state æ›´æ–°ç‰‡æ®µ</mark>
    ```py
    {'topic': 'ice cream and cats', 'joke': ''}
    {'topic': 'ice cream and cats', 'joke': 'This is a joke...'}
    ```
  + `messages`ï¼š<mark>chuck æµ</mark>
    > æ¯å€‹ chuck éƒ½æœ‰å°æ‡‰çš„ metadataï¼Œå¯ä»¥æª¢æŸ¥æ˜¯å“ªå€‹ node é€å‡ºçš„ chunk æµ)
  + `custom`ï¼šä½ æƒ³ä¸»å‹•é€šçŸ¥çš„å…§å®¹
  + `debug`ï¼šè¶…è©³ç´° debug è³‡è¨Š
+ ç¯„ä¾‹
  ```py
  from dataclasses import dataclass
  from typing import Any, cast

  from langchain_core.messages import AIMessageChunk
  from langgraph.graph import START, StateGraph
  from langgraph.graph.state import CompiledStateGraph

  from core.model import DummyChatModel

  MODEL = DummyChatModel()


  @dataclass
  class MyState:
      query: str
      reply: str = ""


  def call_model(state: MyState):
      resp = MODEL.invoke(state.query)
      return {"reply": resp.content}


  def test_updates(graph: CompiledStateGraph):
      for chunk in graph.stream(
          MyState(query="Hello World and Goodbye"),
          stream_mode="updates",
      ):
          print(chunk)
          # {'call_model': {'reply': '[echo] Hello World and Goodbye'}}


  def test_values(graph: CompiledStateGraph):
      for chunk in graph.stream(
          MyState(query="Hello World and Goodbye"),
          stream_mode="values",
      ):
          print(chunk)
          # {'query': 'Hello World and Goodbye', 'reply': ''}
          # {'query': 'Hello World and Goodbye', 'reply': '[echo] Hello World and Goodbye'}


  def test_messages(graph: CompiledStateGraph):
      for chunk, metadata in graph.stream(
          MyState(query="Hello World and Goodbye"),
          stream_mode="messages",
      ):
          chunk = cast(AIMessageChunk, chunk)
          metadata = cast(dict[str, Any], metadata)
          print(chunk.content)
          # [echo]~
          # Hello~
          # World~
          # and~
          # Goodbye~


  if __name__ == "__main__":
      graph = StateGraph(MyState).add_node("call_model", call_model).add_edge(START, "call_model").compile()
      test_messages(graph)

  ```

## Interrupts

### patterns

+ Approve or Rejectï¼ˆäººå·¥æ‰¹å‡†ï¼‰
  + ç¯„ä¾‹
    ```py
    from typing import Literal

    from langchain_core.runnables.config import RunnableConfig
    from langgraph.checkpoint.memory import InMemorySaver
    from langgraph.graph import END, START, StateGraph
    from langgraph.types import Command, interrupt
    from typing_extensions import TypedDict


    class State(TypedDict):
        status: str


    def approval_node(state: State) -> Command[Literal["ok", "cancel"]]:
        approved = interrupt("Do you approve this?")
        return Command(goto="ok" if approved else "cancel")


    def ok_node(state: State) -> State:
        return {"status": "approved"}


    def cancel_node(state: State) -> State:
        return {"status": "rejected"}


    def create_workflow():
        builder = StateGraph(State)
        builder.add_node("approval", approval_node)
        builder.add_node("ok", ok_node)
        builder.add_node("cancel", cancel_node)
        builder.add_edge(START, "approval")
        builder.add_edge("ok", END)
        builder.add_edge("cancel", END)
        graph = builder.compile(checkpointer=InMemorySaver())
        return graph


    if __name__ == "__main__":
        graph = create_workflow()
        config: RunnableConfig = {"configurable": {"thread_id": "t1"}}
        initial = graph.invoke({"status": "pending"}, config=config)
        interrupt_message = initial["__interrupt__"]
        decision = {
            "y": True,
            "n": False,
        }.get(input(f"{interrupt_message[0].value} ").lower(), False)
        final = graph.invoke(Command(resume=decision), config=config)
        print(final)
    ```
+ Review and Edit Stateï¼ˆäººå·¥æ‰¹æ”¹ï¼‰
  + ç¯„ä¾‹
    ```py
    from langchain_core.runnables.config import RunnableConfig
    from langgraph.checkpoint.memory import MemorySaver
    from langgraph.graph import END, START, StateGraph
    from langgraph.types import Command, interrupt
    from typing_extensions import TypedDict


    class ReviewState(TypedDict):
        generated_text: str


    def review_node(state: ReviewState):
        edited_content = interrupt(
            {
                "instruction": "Review and edit this content",
                "content": state["generated_text"],
            }
        )

        return {"generated_text": edited_content}


    def create_workflow():
        builder = StateGraph(ReviewState)
        builder.add_node("review", review_node)
        builder.add_edge(START, "review")
        builder.add_edge("review", END)
        graph = builder.compile(checkpointer=MemorySaver())
        return graph


    if __name__ == "__main__":
        graph = create_workflow()
        config: RunnableConfig = {"configurable": {"thread_id": "review-demo-1"}}
        initial = graph.invoke({"generated_text": "Ths is teh frst vershon."}, config=config)
        print("Interrupt from node:")
        print(initial["__interrupt__"])

        edited_text = "This is the first version."
        final_state = graph.invoke(Command(resume=edited_text), config=config)
        print("\nFinal state after human edit:")
        print(final_state)

    ```
+ Interrupts in tools ï¼ˆèª¿ç”¨å·¥å…·å‰ä¸­æ–·ï¼‰
  + ç¯„ä¾‹
    ```py
    from langchain.tools import tool
    from langgraph.types import interrupt

    @tool
    def send_email(to: str, subject: str, body: str):
        """Send an email to a recipient."""

        # Pause before sending; payload surfaces in result["__interrupt__"]
        response = interrupt({
            "action": "send_email",
            "to": to,
            "subject": subject,
            "body": body,
            "message": "Approve sending this email?"
        })

        if response.get("action") == "approve":
            # Resume value can override inputs before executing
            final_to = response.get("to", to)
            final_subject = response.get("subject", subject)
            final_body = response.get("body", body)
            return f"Email sent to {final_to} with subject '{final_subject}'"
        return "Email cancelled by user"
    ```
+ Validating human inputï¼ˆé©—è­‰è¼¸å…¥ï¼‰
  + ç¯„ä¾‹
    ```py
    from langgraph.types import interrupt

    def get_age_node(state: State):
        prompt = "What is your age?"

        while True:
            answer = interrupt(prompt)  # payload surfaces in result["__interrupt__"]

            # Validate the input
            if isinstance(answer, int) and answer > 0:
                # Valid input - continue
                break
            else:
                # Invalid input - ask again with a more specific prompt
                prompt = f"'{answer}' is not a valid age. Please enter a positive number."

        return {"age": answer}
    ```

### pitfalls

+ Do not wrap interrupt calls in try/except
  > å› ç‚º `interrupt()` æ˜¯é€éä¸Ÿå‡º `InternalInterruptException` ä¾†æš«åœ graphï¼Œè‹¥ä½ ç”¨ try/except æŠŠå®ƒåŒ…èµ·ä¾†ï¼Œå°±æœƒæŠŠé€™å€‹ä¾‹å¤–åƒæ‰ã€‚
  ```py
  def node(state):
      try:
          answer = interrupt("What is your name?") # âŒ
      except Exception:
          print("Error!")
  ```
+ Do not reorder interrupt calls within a node
  > æ¯ä¸€æ¬¡ resumeï¼ŒLangGraph æœƒæ ¹æ“šã€Œinterrupt å‡ºç¾çš„é †åºã€å–ç”¨ resume å€¼ã€‚å¦‚æœ interrupt çš„ã€Œé †åºæ”¹è®Šã€æˆ–ã€Œæœ‰æ™‚å€™åŸ·è¡Œã€æœ‰æ™‚å€™ä¸åŸ·è¡Œã€ï¼Œresume å°±æœƒèˆ‡ interrupt ä½ç½® mismatchã€‚
  ```py
  def node(state):
      name = interrupt("What is your name?")
      if state.get("needs_age"):
          age = interrupt("What is your age?") # âŒ
      city = interrupt("What is your city?")
  ```
+ Do not pass complex values to interrupt
  > interrupt payload å¿…é ˆæ˜¯ JSON å¯åºåˆ—åŒ–çš„è³‡æ–™ï¼ˆå› ç‚ºæœƒå¯«å…¥ checkpointerï¼‰ã€‚
  ```py
  def node(state):
      response = interrupt({
          "question": "Enter data",
          "validator": lambda x: True # âŒ
      })
  ```
+ Side effects before interrupt must be idempotent
  > <mark>å› ç‚º resume æ™‚ï¼ŒLangGraph æœƒé‡æ–°åŸ·è¡Œæ•´å€‹ nodeï¼Œä¸æ˜¯å¾ interrupt é‚£ä¸€è¡ŒçºŒè·‘</mark>ã€‚å› æ­¤ä»»ä½• interrupt ä¹‹å‰çš„å‹•ä½œæœƒè¢«é‡è¤‡åŸ·è¡Œã€‚
å¦‚æœä¸æ˜¯ idempotent â†’ æœƒé€ æˆå‰¯ä½œç”¨è¢«åŸ·è¡Œå…©æ¬¡ä»¥ä¸Šã€‚
  ```
  def node(state):
      db.insert_log("pending_approval")  # âŒ
      approved = interrupt("Approve?")
      ...
  ```


## Semantic Search

### 1. [document loaders](https://docs.langchain.com/oss/python/integrations/document_loaders)
+ èªªæ˜
  + è¼‰å…¥å„ç¨®æª”æ¡ˆ (Webpageã€PDFã€Cloud)
+ ç¯„ä¾‹
  ```py
  ...
  ```

### 2. [text splitters](https://docs.langchain.com/oss/python/integrations/splitters)
+ å¥—ä»¶ï¼š`langchain-text-splitters`
+ èªªæ˜
  + LLM æœ‰ context window ä¸Šé™
  + <mark>å¤§æª”æ¡ˆéœ€è¦åœ¨é€å…¥ LLM å‰ï¼Œåˆ†å‰²æˆåˆç†å¤§å°çš„ chunks</mark>
  + è¼¸å…¥ï¼šæœªåˆ†å‰²çš„å¤šå€‹ Documents
  + è¼¸å‡ºï¼šåˆ†å‰²å®Œæˆçš„å¤šå€‹ Documents
+ æ ¸å¿ƒç›®æ¨™
  + å°‡æ–‡ä»¶åˆ‡æˆ chunks
  + ä¿ç•™èªæ„é€£è²«åº¦
  + è®“ chunk é©åˆæ¨¡å‹çš„ context é™åˆ¶
  + æå‡æª¢ç´¢ã€RAGã€æ‘˜è¦çš„æ•ˆæœ
+ åˆ†å‰²ç­–ç•¥
  + text structure-basedï¼ˆæ–‡æœ¬çµæ§‹å¼åˆ†å‰²ï¼‰[âœ… æœ€å¸¸ç”¨]
    > `RecursiveCharacterTextSplitter`\
    > åˆ©ç”¨æ–‡æœ¬çš„è‡ªç„¶éšå±¤ï¼šæ®µè½ â†’ å¥å­ â†’ å–®å­—\
    > å…ˆè©¦è‘—ä¿æŒè¼ƒå¤§çš„çµæ§‹ (paragraph)\
    > å¦‚æœå¤ªå¤§ (è¶…é chunck size)ï¼Œå°±é™åˆ°æ›´å°çš„å±¤ç´š (sentence)\
    > å†ä¸è¡Œï¼Œå°±é™åˆ°æ›´ç´° (word or character)
    ```py
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=50,
        chunk_overlap=20,
        add_start_index=True,
    )

    all_splits = text_splitter.split_documents(docs)
    ```
  + length-basedï¼ˆé•·åº¦å¼åˆ†å‰²ï¼‰
    > `CharacterTextSplitter`\
    > ä¾æ“šæ–‡å­—æ•¸é‡åˆ†å‰²ï¼š\
    > Token-based â†’ ä¿è­‰ç¬¦åˆæ¨¡å‹ token é™åˆ¶\
    > Character-based â†’ å°å¤šèªè¨€è¼ƒç©©å®šï¼ˆä¸­æ–‡å­—ä½”ä¸€å€‹å­—å…ƒä½†å¯èƒ½æ˜¯å¤š tokenï¼‰
  + document structure-basedï¼ˆæ–‡ä»¶çµæ§‹å¼åˆ†å‰²ï¼‰
    > åˆ©ç”¨æ–‡ä»¶çš„æ—¢æœ‰çµæ§‹å»åˆ‡ï¼Œèƒ½ä¿ç•™èªæ„çµ„ç¹”ï¼š\
    > Markdownï¼šç”¨ #, ##, ### (`MarkdownHeaderTextSplitter`)\
    > HTMLï¼šç”¨ HTML tag (`HTMLHeaderTextSplitter`)\
    > JSONï¼šæ¯å€‹ object / array item (`RecursiveJsonSplitter`)\
    > codeï¼šå‡½å¼ã€classã€block (`RecursiveCharacterTextSplitter.from_language`)
+ ç¯„ä¾‹
  ```py
  ...
  ```

### 3. [vector store](https://docs.langchain.com/oss/python/integrations/vectorstores/pgvectorstore)
+ èªªæ˜
  + <mark>æˆ‘æ„› Postgres</mark>
+ ç¯„ä¾‹
  ```py
  import asyncio
  import sys
  from typing import Any

  from langchain_core.embeddings import Embeddings
  from langchain_core.vectorstores import VectorStore
  from langchain_openai import OpenAIEmbeddings
  from langchain_postgres import PGEngine, PGVectorStore


  def create_embedding_model(model_name: str = "text-embedding-3-small"):
      return OpenAIEmbeddings(model=model_name)


  def get_vector_size(embedding: Embeddings) -> int:
      return len(embedding.embed_query("hello"))


  def create_pg_engine(url: str) -> PGEngine:
      return PGEngine.from_connection_string(url=url)


  async def init_vector_table(pg_engine: PGEngine, table_name: str, vector_size: int):
      try:
          await pg_engine.ainit_vectorstore_table(
              table_name=table_name,
              vector_size=vector_size,
          )
      except Exception:
          pass  # table å·²å­˜åœ¨å‰‡ç•¥é


  async def create_vector_store(
      pg_engine: PGEngine,
      table_name: str,
      embedding: Embeddings,
  ):
      return await PGVectorStore.create(
          engine=pg_engine,
          table_name=table_name,
          embedding_service=embedding,
      )


  async def insert_texts(
      vector_store: VectorStore,
      texts: list[str],
      metadatas: list[dict[str, Any]],
  ):
      return await vector_store.aadd_texts(texts=texts, metadatas=metadatas)


  async def search_similar(
      vector_store: VectorStore,
      query: str,
      k: int = 3,
  ):
      # similarity_search å¯ä»¥æŒ‡å®š filter (âœ… æ‡‰è©²æ˜¯å¾ metadata æ‰¾ï¼Œå¾…é©—è­‰)
      return await vector_store.asimilarity_search(query=query, k=k)


  async def main():
      # è©åµŒå…¥æ¨¡å‹åˆå§‹åŒ–
      embedding = create_embedding_model()
      vector_size = get_vector_size(embedding)

      # Postgres è³‡æ–™åº«
      pg_engine = create_pg_engine("postgresql+psycopg://postgres:postgres123@localhost:5432/langchain")
      await init_vector_table(pg_engine, "test_vectors", vector_size)
      vector_store = await create_vector_store(pg_engine, "test_vectors", embedding)
      print("PGVectorStore ready!")

      # å°‡ chunk åµŒå…¥æˆ vector
      texts = [
          "é«˜é›„ä»Šå¤©æœƒä¸‹é›¨å—ï¼Ÿ",
          "ä»€éº¼æ˜¯ LangGraph çš„ interruptï¼Ÿ",
          "pgvector æ˜¯ PostgreSQL çš„å‘é‡æ“´å……å¥—ä»¶ã€‚",
      ]
      metadatas = [{"id": i} for i in range(len(texts))]
      await insert_texts(vector_store, texts, metadatas)

      # æŸ¥è©¢
      results = await search_similar(vector_store, "LangChain å¾ˆæœ‰è¶£", k=1)
      print("æŸ¥è©¢çµæœï¼š")
      for doc in results:
          print("-", doc.page_content, doc.metadata)
          # ä»€éº¼æ˜¯ LangGraph çš„ interruptï¼Ÿ


  if __name__ == "__main__":
      if sys.platform == "win32":
          asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
      asyncio.run(main())

  ```
  ```bash
  # docker é…ç½®
  docker run -d \
    --name pgvector \
    -e POSTGRES_PASSWORD=postgres123 \
    -e POSTGRES_USER=postgres \
    -e POSTGRES_DB=langchain \
    -p 5432:5432 \
    pgvector/pgvector:pg18
  ```

### 4. retriever
+ èªªæ˜
  + <mark>æŠŠ vector store åŒ…è£æˆ Runnable çš„ adaptor</mark>
+ ç¯„ä¾‹
  ```py
  from typing import List

  from langchain_core.documents import Document
  from langchain_core.runnables import chain


  @chain
  def retriever(query: str) -> List[Document]:
      return vector_store.similarity_search(query, k=1)


  retriever.batch(
      [
          "How many distribution centers does Nike have in the US?",
          "When was Nike incorporated?",
      ],
  )
  ```
  ```py
  retriever = vector_store.as_retriever(
      search_type="similarity",
      search_kwargs={"k": 1},
  )

  retriever.batch(
      [
          "How many distribution centers does Nike have in the US?",
          "When was Nike incorporated?",
      ],
  )
  ```


## Config

### [langgraph.json](https://docs.langchain.com/oss/python/langgraph/application-structure)
```py
...
```



## Bug

```py
import operator
from dataclasses import dataclass, field
from typing import Annotated

from langgraph.graph import END, START, StateGraph
from langgraph.types import Overwrite


def my_mul(a, b):
    return a * b


def my_add(a, b):
    return a + b


def unveil(a, b):
    print(a, b)
    pass


@dataclass
class MetaInfo:
    notes: list[str]
    version: int


@dataclass
class OrderState:
    total_price: float = 0  # è¦†è“‹ï¼ˆé è¨­ï¼‰
    item_count: Annotated[int, my_add] = 0  # ç´¯åŠ 
    discount_multiplier: Annotated[float, my_mul] = 1.0  # ç´¯ä¹˜
    meta: Annotated[MetaInfo, unveil] = field(default_factory=lambda: MetaInfo(notes=[], version=888))


def add_items(state: OrderState) -> OrderState:
    return OrderState(
        total_price=200,
        item_count=2,
        discount_multiplier=0.9,
    )


def add_more_items(state: OrderState) -> OrderState:
    return OrderState(
        total_price=10,
        item_count=1,
        discount_multiplier=0.8,
    )


def finalize(state: OrderState) -> OrderState:
    return OrderState(
        total_price=state.total_price * state.discount_multiplier,
        item_count=0,
        discount_multiplier=1.0,
    )


if __name__ == "__main__":
    builder = StateGraph(OrderState)

    builder.add_node("add_items", add_items)
    builder.add_node("add_more_items", add_more_items)
    builder.add_node("finalize", finalize)

    builder.add_edge(START, "add_items")
    builder.add_edge("add_items", "add_more_items")
    builder.add_edge("add_more_items", "finalize")
    builder.add_edge("finalize", END)

    graph = builder.compile()

    input_state = OrderState(
        total_price=0,
        item_count=5,
        discount_multiplier=2.0,
        meta=MetaInfo(notes=["Hello"], version=2),
    )

    output = graph.invoke(input_state)
    print(output)
```